{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mottaquikarim/pycontent/blob/master/.out/topics/data_cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "<img src=\"https://i.chzbgr.com/full/1898496256/h42C0CC42/panda-cleaning-instructions\" style=\"margin: 0 auto; float: right;\"/>\n",
    "\n",
    "Data cleaning is arguably as important as any amount of insight you obtain from your dataset. The more data there is, especially data aggregated from multiple sources, the messier it is. You need to reformat and standardize it before you can successfully complete any real analysis. Otherwise...garbage in, garbage out...\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Handling null values\n",
    "* Element-wise functions\n",
    "* Vectorized typecasting\n",
    "* Scaling variables\n",
    "* Series.map()\n",
    "* Series.apply()\n",
    "* Row- & Column-wise functions \n",
    "* DataFrame.apply()\n",
    "\n",
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load the data with `imdbID` as the index and make a copy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omdb_orig = pd.read_csv('https://raw.githubusercontent.com/mottaquikarim/pycontent/master/content/raw_data/omdb4500_cleaning.csv', index_col='imdbID')\n",
    "movies = omdb_orig.copy()\n",
    "print('data loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A Note on Handling Null Values\n",
    "\n",
    "Having null values in your data can cause various issues with your code and with your analysis. Deciding how to deal with null values is a huge part of cleaning your data, and you have to think about each column contextually. At a high level, you can drop rows/columns containing null values or replace all instances of null values with some default value (e.g. the mean, the value with the highest frequency, \"Unknown\", etc.). \n",
    "\n",
    "In addition to the \"how\", you also have to contextually consider the \"when\". For example, we had to get rid of all rows containing TV shows before we could sort by year because those rows contained non-numeric values such as \"2009-2017\". In this lesson, we'll be dropping rows with null values as we explore different concepts and consider different variables.\n",
    "\n",
    "## Element-wise Functions\n",
    "\n",
    "An **elementwise** function is one that you call on a Series object as a whole, but that vectorizes the functions actions across each of the Series elements. \n",
    "\n",
    "### Typecasting\n",
    "\n",
    "Typecasting a Series is one of the most basic elementwise functions. Most commonly in cleaning your data, you'll use:\n",
    "\n",
    "* `pd.to_numeric(s)`: typecast the items in a Series to ints or floats; will infer which numeric type is best\n",
    "* `s.astype()`: typecast the items in a Series to some data type; accepts `'int64'`, `'float64'`, `'str'`, etc.\n",
    "\n",
    "Let's test these out quickly on the `Year` column. What data type is it now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = movies['Year'].copy()\n",
    "print(type(test_year[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convert it to string type using `.astype()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = test_year.astype('str')\n",
    "type(test_year[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convert it to one of the numeric types using `pd.to_numeric(s)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = pd.to_numeric(test_year)\n",
    "type(test_year[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The `.map()` function\n",
    "\n",
    "For the next few example, we'll leverage the `s.map(arg, na_action=None)` function, another **elementwise** function. You can use the `.map(arg, na_action=None)` function to substitute or transform each value in a Series with another value. `.map()` itself serves to pass along \"instructions\" for how to manipulate each element in the Series. Accordingly, the `arg` parameter will accept single-argument functions, dicts, or Series. As you might imagine, `.map()` requires us to pass it a \"mapping\" for the before and after values.\n",
    "\n",
    "\n",
    "| Type of `arg` |    Map From   |    Map To    |\n",
    "|:-------------:|:-------------:|:------------:|\n",
    "|   Function    |  1 Parameter  | Return Value |\n",
    "|     Dict      |      Key      |     Value    |\n",
    "|    Series     |     Index     |     Value    |\n",
    "\n",
    "\n",
    "In *most* cases, if there are null values in the original Series, an error will stop your `.map()` function's execution. (We'll see the exception soon.) The `na_action` parameter allows you to bypass this issue until you decide what how to handle different pieces of missing data in your dataset. If you set `na_action='ignore'`, `.map()` will simply skip over null values.\n",
    "\n",
    "Finally, *notice that there is no `inplace` parameter for `.map()`*. You have to remember to assign the results to some variable, or you'll never see them!\n",
    "\n",
    "### Reformat Rotten Tomatoes\n",
    "\n",
    "If we look at the three movie rating variables, each source has provided ratings for each movie on a different scale and in a different format. \n",
    "\n",
    "* `imdbRating`: 0.0-10.0; float format\n",
    "* `Metascore`: 0.0-100.0; float format\n",
    "* `Rotten Tomatoes`: 0-100%; string format\n",
    "\n",
    "Let's start by mapping `Rotten Tomatoes` ratings from strings to numeric values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Rotten Tomatoes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All we have to do is strip off the \"%\" character and typecast the values to floats. Of course, we have to pass this to `.map()` in the form of a function. For brevity, whenever possible, most people use **lambda functions** with `.map()`. A **lambda function** is a nameless function that is defined, used, and forgotten in one line. Here's an example of the syntax relative to a regular function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "...is equivalent to...\n",
    "\n",
    "lambda x: x**2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The body of the lambda function we need for reformatting `Rotten Tomatoes` ratings is `float(x.strip('%'))`. Also, don't forget to set `na_action='ignore'`, since there are null values in this column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Rotten Tomatoes'] = movies['Rotten Tomatoes'].map(lambda x: float(x.strip('%')), na_action='ignore')\n",
    "movies['Rotten Tomatoes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scaling Variables\n",
    "\n",
    "Look again at the formats for our ratings variables:\n",
    "\n",
    "* `imdbRating`: 0.0-10.0; float format\n",
    "* `Metascore`: 0.0-100.0; float format\n",
    "* `Rotten Tomatoes`: 0.0-100.0; float format\n",
    "\n",
    "For graphical comparisons, you always want numeric variables on the same *scale*. Since it's easier to see minute differences between data points on a larger scale, we'll scale `imdbRating` to match `Metascore` and `Rotten Tomatoes`.\n",
    "\n",
    "First, how many movies are missing a rating from IMDb?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['imdbRating'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are 5 null values, so we need to set `na_action='ignore'`, right? Nope! Here's the exception to `.map()`'s rule about null values. \n",
    "\n",
    "Assume `a = np.nan` (`np.nan` is the notation for a null value):\n",
    "\n",
    "* `a.split(',')` would raise an error because you can't apply that, or any, method or function to a null value\n",
    "* `a*10` will NOT raise an error because *basic mathematical operators* treat null values like 0s\n",
    "\n",
    "Knowing this, we could easily scale `imdbRating` with `movies['imdbRating']*10`, but let's use this opportunity to prove the null value exception with `.map()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['imdbRating'] = movies['imdbRating'].map(lambda x: x*10)\n",
    "movies['imdbRating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Element-wise Functions with .apply()\n",
    "\n",
    "When applied to a Series object, the `.apply()` function is effectively the same as `.map()`. It's just another elementwise function. The difference is that you can pass it more complex functions (e.g. more than one line, conditionals, error handling, etc.), while `.map()` is mainly paired with simple lambda functions.\n",
    "\n",
    "* `s.apply()`\n",
    "\n",
    "As with `.map()`, if there are null values in the Series, an error will stop the code's execution. However, `.apply()` has no equivalent to the `na_action` parameter in `.map()`. If you don't want to drop all the rows with null values just to get your `.apply()` function working, you can **manually** skip over null values using the same logic behind the `na_action` parameter. For example, you can build in conditional logic or a try/except statement.\n",
    "\n",
    "### Reformat imdbVotes\n",
    "\n",
    "`imdbVotes` also needs to be a numeric variable, but it would take more than one line to accomplish this. Therefore, it wouldn't be as efficient to use `.map()`. Instead, we can define our own function and pass it to `.apply()`.\n",
    "\n",
    "First, make a temporary copy of the `imdbVotes` column to use with our `.apply()` operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_imdbVotes = movies['imdbVotes'].copy()\n",
    "temp_imdbVotes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When you define a custom function to use with `.apply()`, it's always a good idea to test the function on a single value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def votes_reformat(value):\n",
    "    \"\"\"remove commas from str and convert field to int\"\"\"\n",
    "    try:\n",
    "        split = value.split(',')\n",
    "        votes = int(''.join(split))\n",
    "        return votes\n",
    "    except Exception as e:\n",
    "        return value\n",
    "\n",
    "test = temp_imdbVotes[0]\n",
    "votes_reformat(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The single-value test worked, so we'll run it on the whole column...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_imdbVotes = temp_imdbVotes.apply(votes_reformat)\n",
    "temp_imdbVotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Wait... they're floats, not ints... And if you try to typecast directly to integers using `.astype('int64)`, it will cause a `TypeError`. Why? This is because the null values. In Pandas, `NaN` is considered a float. Since Series object must have homogenous data types, any numeric Series containing null values will be forced to `dtype='float64'`.\n",
    "\n",
    "When we decide how we want to handle the null values across our ratings fields, we can re-typecast this column. For now, we'll just reassign `temp_imdbVotes` back to the `movies` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['imdbVotes'] = temp_imdbVotes\n",
    "movies['imdbVotes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reformat Runtime\n",
    "\n",
    "Next, we'll repeat this process with `Runtime`. This time though, we'll look at the column's null values first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_runtime = movies[pd.isnull(movies['Runtime'])]\n",
    "print(len(missing_runtime))\n",
    "missing_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are only three rows missing data for `Runtime`, all of which are also missing ratings from Rotten Tomatoes and Metascore. We might as well drop these rows, but we don't need to grab each one's index this time. (Flash back to when we removed TV shows using `movies.drop(labels=non_movie_ids, axis=0)`.) Since these three are the only rows with null values for `Runtime`, we can drop them as a group using `.dropna()`.\n",
    "\n",
    "* `df.dropna(axis=0, how='any', subset=[col1], inplace=False)`\n",
    "\n",
    "When you're dropping rows (i.e. axis=0), the `subset` parameters indicates which columns to check for null values. Accordingly, if you're checking for duplicates in multiple columns, the `how` parameters indicates whether you want the function to drop the row if `'any'` of those columns contain a null value or only if `'all'` of them are null.\n",
    "\n",
    "Let's drop all rows that contain a null value in the `Runtime` column from the `movies` dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.dropna(subset=['Runtime'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Did it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Runtime'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now when we reformat `Runtime`, there won't be any null values forcing the rest of the Series values into `float` format.\n",
    "\n",
    "Make a temporary copy of the `Runtime` column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_runtime = movies['Runtime'].copy()\n",
    "temp_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define and test a custom function to remove `' min'` from each value and typecast it to an integer. By the way, even though we just dropped the rows with null values, we should still build in a try/except statement to catch other potential issues!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime_reformat(value):\n",
    "    \"\"\"remove 'min' from str and convert field to int\"\"\"\n",
    "    try:\n",
    "        value = value.split(' ')\n",
    "        numeric_runtime = int(value[0])\n",
    "        return numeric_runtime\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return value\n",
    "\n",
    "test = temp_runtime[0]\n",
    "result = runtime_reformat(test)\n",
    "\n",
    "# TESTING ONE VALUE...\n",
    "print(f'''\n",
    "BEFORE: {test}, {type(test)}\n",
    "AFTER: {result}, {type(result)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let it run on the whole Series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_runtime = temp_runtime.apply(runtime_reformat)\n",
    "temp_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Assign it back to the `movies` dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Runtime'] = temp_runtime\n",
    "movies['Runtime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Filter/Drop Shorts\n",
    "\n",
    "In the last lesson, we dropped all TV shows from the dataframe because we only want to evaluate movies. In the same vein, it's not truly accurate to compare long-form movies to \"short-form videos\". That might include [animated shorts from Pixar](https://www.studiobinder.com/blog/pixar-shorts/), for example, or \"made-for-TV\" specials that last ~40-45 minutes (1 hour with commercials).\n",
    "\n",
    "How many \"shorts\" are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorts = movies[movies['Runtime'] < 45].copy()\n",
    "shorts.sort_values(by=['Runtime'], ascending=False, inplace=True)\n",
    "\n",
    "print(len(shorts))\n",
    "shorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Notice that the rows with null values are NOT included here!*\n",
    "\n",
    "Drop these by grabbing their index labels and check to make sure they're gone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorts_idx = list(shorts.index)\n",
    "movies.drop(labels=shorts_idx, axis=0, inplace=True)\n",
    "shorts = movies['Runtime'] < 45\n",
    "shorts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BONUS: Row- & Column-wise Functions with .apply()\n",
    "\n",
    "You can also implement `.apply()` as dataframe method. In this context, `.apply()` is a **row-wise** or **column-wise** function. Here's the difference:\n",
    "\n",
    "* **`s.apply(func)`** dynamically changes each value of a Series; in the context of a dataframe, it's like passing all the rows but only a single column to the function\n",
    "* **`df.apply(func, axis=0)`** dynamically changes each value *of each row/column* of a dataframe; when axis = 1, passes *entire rows* to the function\n",
    "\n",
    "Of course, the `axis` parameter is what determines whether your function is row-wise or column-wise. However, it's a little counter-intuitive. We know that `axis 0` refers to rows and `axis 1` refers to columns, but in the context of `df.apply()`:\n",
    "\n",
    "* If `axis=0`, the objects passed to `func` will be *a Series containing the dataframe's COLUMNS*. The changes will be made to each value (i.e. column) in the set of columns.\n",
    "* If `axis=1`, the objects passed to `func` will be *a Series containing the dataframe's ROWS*. The changes will be made to each value (i.e. column) in the set of rows.\n",
    "\n",
    "### Languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_lang = movies[pd.isnull(movies['Languages'])].copy()\n",
    "print(movies['Languages'].isnull().sum())\n",
    "null_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are 4 movies with `NaN` in their `Languages` field. But do you notice anything? The first movie with sound was The Jazz Singer, released in 1927. All four of these movies were released before that year. \n",
    "\n",
    "* `.fillna(value=None, inplace=False)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Languages'].fillna(value='Silent', inplace=True)\n",
    "movies.loc[null_lang.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What if other early movies were incorrectly labeled? If we want to be consistent, we need to check all the movies made before 1927 and set those to \"Silent\" as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silent_films = movies[movies['Year'] < 1927]\n",
    "silent_films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As expected, there are a few. Let's write a function to change that. We'll want to test two values here: one movie made before 1927 and one after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silent_lang(row):\n",
    "    try:\n",
    "        if row['Year'] < 1927:\n",
    "            row['Languages'] = 'Silent'\n",
    "            return row\n",
    "        else:\n",
    "            return row\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return row\n",
    "\n",
    "temp = movies.copy()\n",
    "\n",
    "pos_test = silent_lang(temp.loc['tt0013442'].copy()) # Nosferatu\n",
    "pos_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test = silent_lang(temp.iloc[0].copy())\n",
    "neg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The tests worked, so it's safe to apply it to the dataframe as a whole. Check to see if those silent films changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.apply(silent_lang, axis=1)\n",
    "temp.loc[silent_films.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember to reassign this new dataframe back to the `movies` variable before moving on! Now look at all the movies made before 1927.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = temp.copy()\n",
    "movies[movies['Year'] < 1927]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Compound Filtering & Handling Null Movie Ratings \n",
    "\n",
    "We can probably uncover some interesting insights by comparing different characteristics of movies to different ratings sources. For movies missing one or more ratings, we can always replace those null values with the mean of the column. However, a strict statistician would tell you that this distorts the accuracy of your results. And when there's money riding on the outcomes of your analyses, you probably want to aim as close to the bull's eye as possible.\n",
    "\n",
    "Ultimately, we want to know what proportion of movies is missing at least one rating. If it's too large, we might consider dropping one of the rating sources. The count of non-null values per column that we see with `.info()` will NOT answer this question for us. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We don't know whether each movie is missing one, both, or neither of the ratings from Rotten Tomatoes and Metacritic. We have to use compound filtering to determine the proportion of movies with complete ratings data vs. the proportion missing at least one rating. First, we need the total number of movies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_rows = len(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we need to write a compound filter to return all the movies that:\n",
    "\n",
    "* have a rating from IMDb *AND*\n",
    "* have a rating from Rotten Tomatoes *AND*\n",
    "* have a rating from Metacritic\n",
    "\n",
    "**NOTE!** When implementing compound filters, you have to use `&` and `|` instead of `and` and `or` respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ratings = movies[pd.notnull(movies['imdbRating']) & pd.notnull(movies['Rotten Tomatoes']) & pd.notnull(movies['Metascore'])]\n",
    "\n",
    "p1 = len(full_ratings)\n",
    "print(f'{p1} movies, or', '{:.2%},'.format(p1/whole_rows), 'have ratings from all 3 sources.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we need to write a compound filter to return all the movies that:\n",
    "\n",
    "* are missing a rating from IMDb *OR*\n",
    "* are missing a rating from Rotten Tomatoes *OR*\n",
    "* hare missing a rating from Metacritic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rating = movies[pd.isnull(movies['Rotten Tomatoes']) | pd.isnull(movies['Metascore']) | pd.isnull(movies['imdbRating'])]\n",
    "\n",
    "p2 = len(missing_rating)\n",
    "print(f'{p2} movies, or', '{:.2%},'.format(p2/whole_rows), 'are missing a rating from at least one source.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A good gut check to make sure your results line up with what you expect is to run this conditional:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 + p2 == whole_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A significant chunk of these movies are missing at least one rating, but even if we remove those, we still have a relatively large sample size. As such, we'll drop all the rows missing a rating. \n",
    "\n",
    "Notice how we pass `how='any'` *as well as* a subset of columns. This very specifically drops all the rows where *any column in the subset* contains a null value. After that, we'll be able to typecast the `imdbVotes` column to `int64` because there won't be any null values to stop us!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.dropna(axis=0, how='any', subset=['imdbRating', 'Rotten Tomatoes', 'Metascore'], inplace=True)\n",
    "movies['imdbVotes'] = movies['imdbVotes'].astype('int64')\n",
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Missing Qualitative Data \n",
    "\n",
    "With qualitative or categorical data, you can either drop null values or fill them with some version of an \"Unknown\" category. How you determine that value is really subjective.\n",
    "\n",
    "Let's see how many movies are missing data about their **actors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_actors = movies[pd.isnull(movies['Actors'])]\n",
    "print(f'{len(missing_actors)} movies are missing actor info.\\n')\n",
    "missing_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ALL of these are documentaries. Documentaries shouldn't theoretically have actors, so we can use `.fillna()` to update all of these with \"None (Documentary)\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Actors'].fillna('None (Documentary)', inplace=True)\n",
    "movies['Actors'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**How about writers?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_writer = movies[pd.isnull(movies['Writer'])]\n",
    "print(f'{len(missing_writer)} movies are missing writer info.\\n')\n",
    "missing_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It looks like a lot of there are documentaries too. We can isolate the ones which aren't documentaries by negating `movies['Genre'].str.contains('Documentary')`. To negate a whole condition, simply preface it with a tilde `~`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_doc_writer = movies[(pd.isnull(movies['Writer'])) & ~movies['Genres'].str.contains('Documentary')]\n",
    "missing_doc_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Only one of them isn't a documentary. I looked up the outlier, so we could add it here. The rest, we can fill with \"Unknown\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc['tt4063178', 'Writer'] = 'Alex Ranarivelo, Ali Afshar'\n",
    "movies['Writer'].fillna('Unknown', inplace=True)\n",
    "movies['Writer'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **Production** companies are a bit more complicated. The values need to be disamiguated, and there are over 150 missing data. So for the purposes of this course, we'll just fill the null values with \"Indie\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Production'].fillna('Indie', inplace=True)\n",
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## New Functions Featured\n",
    "\n",
    "Functions featured include (in order of appearance):\n",
    "* `pd.to_numeric(s)`\n",
    "* `s.astype()`\n",
    "* `s.map(arg, na_action=None)`\n",
    "* `s.apply(func)`\n",
    "* `df.dropna(axis=0, how='any', subset=[col1], inplace=False)`\n",
    "* `.fillna(value=None, inplace=False)`\n",
    "* `df.apply(func, axis=1)`\n",
    "* `s.fillna(value=None, inplace=False)`\n",
    "\n",
    "## ðŸ‹ï¸â€â™€ï¸ **EXERCISES** ðŸ‹ï¸â€â™€ï¸ \n",
    "\n",
    "Practice using these methods in your copy of wrangling_pset2.ipynb in Google Drive."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
